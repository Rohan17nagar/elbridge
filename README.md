# Gerrymandering

## Introduction and Background

### Representation and Districting

#### A Well-Represented Populace

#### The Problem of Gerrymandering

### Computational Approaches Thus Far

#### Intractability Results

#### Quantifying Gerrymandering

#### Drawing Districts

## This Approach

The fundamental problem I sought to solve was this: can a computer, given an appropriate notion of "fairness" of a districting, come up with a districting that is as fair as possible --- that is, as representative of the population it districts as possible? Previous works in computational districting have focused entirely on quantifying gerrymandering (in order to invalidate existing districts) or on drawing districts based solely on first-order characteristics like compactness, often throwing out one requirement (like contiguity) in favor of another.

The approach I take is straightforward. I model states as weighted graphs and reduce the problem of districting a state to the problem of partitioning a graph into equally-weighted, connected subgraphs. In the following sections, I'll motivate and explain the model I use, discuss intractability results of the problem as specified, and explain how I created a straightforward optimization engine.

### The Model

Let's begin with the model. The problem ultimately reduces to one of dividing a state with a known population distribution into q regions that are connected (that is, contiguous) and that have roughly the same population. There are two ways of modeling this problem. In the first, we treat the state as a continuous subset S of the real plane. The population function p can be thought of as a probability function from subsets of S to [0,1], the idea being that p(A u B) = p(A) + p(B) - p(A n B); note that p(empty set) = 0 and p(S) = 1. Tbe goal is therefore to find a set of subsets S* = {S_1, S_2, ..., S_q} of S, such that p(S_1) = p(S_2) = ... = p(S_k).

The primary issue with this model is straightforward -- population is not continuous. The U.S. Census Bureau breaks the country up into geographic units called census blocks, which are small units of land that typically contain no more than 1,000 people (in fact, the vast majority of blocks are empty). So, instead of using a continuous model, I use a discrete one, where my "atomic" units are precisely census blocks. Given this discretization, a graph is the natural model, and indeed is the one I use.

Precisely, I model states as graphs G = (V,E), where V is the collection of census blocks that make up the state and E contains an edge (u,v) if and only if the blocks represented by u and v share a physical border. Each vertex is weighted by the population of the underlying block. Note that this graph is planar, which means that it can be embedded into the plane (i.e., it is possible to draw the graph without ever crossing an edge over another).

It's useful here to define a few terms:

* A *path* between vertices u and v in V is a collection of edges {(v1, v2), (v2, v3), (v3, v4), ..., (vn-1, vn)}, where u = v1 and v = vn. u and v are *connected* if there is a path between them.
* A *connected graph* is a graph where every pair of vertices is connected.
* Given a subset V' of V, the *induced subgraph* on V' is the graph generated by using V' as the vertices and the edges in E between vertices in V' as the edges; formally, the induced subgraph is G_D = (V', E'), where E' = {(u,v) in E | u, v in V'}.
* A *q-partition* of a set S is a set of q subsets of S, S* = {S1, S2, ..., Sq}, such that every s in S is in exactly one element of S*.

Districting a state into q districts can thus be thought of as finding a q-partition of the vertices of the corresponding graph V* such that, for each element Vi of V*, the induced subgraph on Vi is connected, and the sum of the weights of the vertices in each Vi is as close to equal as possible.

### Closed-Form Solutions

The problem as defined above is known as the *balanced connected q-partition* problem, or BCP. It comes in two types: BCP_q, where q is known beforehand, and BCP, where it isn't. (That is, a solution to BCP_q only has to solve the problem for one value of q; a solution to BCP must be able to solve it for all q.) If the graph is unweighted (or, equivalently, every vertex has weight 1), the problem is known as 1-BCP_q (or 1-BCP).

The good news is that the problem has been known since the mid-1970s; the bad news is that for almost every kind of graph, the problem is computationally intractable. Garey and Johnson showed that for general graphs, 1-BCP and 1-BCP_q are NP-complete; Chataigner et al. showed that for general graphs, BCP and BCP_q are NP-hard. Dyer and Frieze showed that for planar and bipartite graphs, 1-BCP and 1-BCP_q are NP-complete. The one kind of graph for which 1-BCP_q and 1-BCP are polynomially solvable are q-connected graphs, where a q-connected graph is a graph that can "tolerate" the removal of at most q-1 vertices before becoming disconnected (or equivalently, q is the size of the smallest vertex set whose removal from the graph would disconnect it). However, Chataigner et al. showed that BCP_q and BCP are still NP-complete for q-connected graphs. (The problem is also relatively hard to approximate; see Chlebikova and Chataigner et al. for some approximability results.)

Note that no result has been shown for planar or bipartite weighted graphs; I give a proof here. `% fill in the rest of the proof`



### Hill-Climbing

Given that BCP is difficult to solve, even when restricted to planar graphs, I instead use an approach known as hill-climbing with random restarts. Basically, the algorithm works as follows:

0. Start from some given position S.
1. Score S. (I'll go into how that works later.)
2. Let N be the set of steps you can take from S0. (I'll define what those are later.) For each step n in N, let S_n be the position you get to by taking n. Find n*, the step such that S_n* has the highest score for all n in N.
3. If S_n* scores higher than S, set S = S_n* and go to step 1. If not, return S.

This algorithm is very fast, but suffers from the fact that it can terminate at local maxima without ever finding global maxima. Other approaches, like simulated annealing, moves to lower-scored positions with probability determined by the total number of steps already made (thus making it more risk-averse over time) and the difference in score, but in addition to being slower are also not guaranteed to find a global maximum. Instead, I alleviate this problem by running the algorithm several times from several randomly-generated positions; thus, as I increase the number of times the algorithm restarts, the probability of finding a global maximum increases.

#### Creating a Starting Configuration

#### Taking a Step

#### Scoring Districtings

### Results

